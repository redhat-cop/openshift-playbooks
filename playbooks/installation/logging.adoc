---
---
= Log Management in OpenShift Enterprise

== Application Logs

=== Accessing Container Logs

Because we use Kubernetes as the container orchestrator for OpenShift, the collection of and access to container logs is actually much simpler than most would think. This is because Kubernetes captures all stdout and stderr from every container and writes it to files on the node as json data within the container's data directory (i.e. `/var/lib/docker/<container-id>/<container-id>-json.log`). Kubernetes then creates a symbolic link to that logfile in /var/log/containers/, the name of which contains metadata about the continer such as namespace, container and pod ids. For example:

----
synthetic-logger-0.25lps-pod_default_synth-lgr-997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b.log
->
/var/lib/docker/containers/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b/997599971ee6366d4a5920d25b79286ad45ff37a74494f262e3bc98d909d0a7b-json.log
----

Awesome! No agents or container customization needed. We just have our application use a stdout/stderr logger (i.e. Console Logger in JBoss) and Kubernetes takes care of the rest.

=== Log Aggregation

Since we already have the container logs available on the Node's file system, we can concentrate on doing something useful with them.

==== Simple Aggregation with Syslog Forwarders

If we just need to collect the raw data, then the centralization process is very simple. We can just point syslog at the directory containing log files and have it forward data on to a link:http://www.itzgeek.com/how-tos/linux/centos-how-tos/setup-syslog-server-on-centos-7-rhel-7.html[central logging server].

.Feedback or Contribution Needed
****
Need some examples of forwarding OpenShift logs using rsyslog.
****

==== Centralizing Logs with Fluentd

The OpenShift 3.0 Documentation provides a way to link:https://docs.openshift.com/enterprise/3.0/admin_guide/aggregate_logging.html[use Fluentd to capture and centralize container logs]. This implementation is fairly limiting though as it doesn't fully parse the log files and capture metadata like namespaces, pods names, etc.

==== Full Metadata Supported Log Aggregation with Fluentd

If we want to collect application metadata so that we can do things like filter logging views based on the application or project, we need to install some additional Fluentd plugins to make our aggregation agent more Kubernetes and Docker aware. Unfortunately this is more complicated at the moment (with OSE 3.0) because Red Hat does not ship fluentd or any of its plugins, so there are no RPMs or installation channels to facilitate installation in an Enterprise Environment.

Here is a guide that walks through installation of fluentd and required plugins using as RubyGems.

link:./logging_with_fluentd{outfilesuffix}[Using Fluentd to Centralize Container Logs]

== System Logs

.Feedback or Contribution Needed
****
Need to add discussions about collecting systemlogs for OpenShift
****

=== OpenShift API Event Logging Through HAProxy

Because all user-attributable events occur through the OpenShift Master API service, we may run into a situation with which each is required to be logged. In a typical multi-master OpenShift setup, each master provides its own API service. This would mean 3 separate API servers to log. Luckily, the front-end proxy server provides a single point of API event aggregation to each of the masters. In this procedure, HAProxy will be configured to provide this level of centralized logging.

By default, OpenShift does not provide logging of events to its API service. Out of the box, HAProxy is configured for TLS passthrough of connections, relying on a simple TCP load balancing of all traffic on port 8443. Normally you would be able to configure HAProxy to HTTP mode and insert a simple httplog statement. This will not work with OpenShift, since it uses SSL certificates for internal communication. This procedures also assumes the OpenShift installation to be in a PKI-enabled environment in which the masters each have their own PKI certificate.

First of all, we need to modify the haproxy config file found in /etc/haproxy/haproxy.conf.

----
global
  chroot  /var/lib/haproxy
  pidfile /var/run/haproxy.pid
  maxconn 20000
  user    haproxy
  group   haproxy
  daemon
  log     127.0.0.1 local1
  tune.ssl.default-dh-param 2048
  
  # turn on stats unix socket
  stats socket /var/lib/haproxy/stats
  
defaults
  mode  tcp
  log   global
  option dontlognull
  option http-server-close
  option forwardfor except 127.0.0.0/8
  option redispatch
  retries 3
  timeout http-request 10s
  timeout queue 1m
  timeout connect 10s
  timeout client 300s
  timeout server 300s
  timeout http-keep-alive 10s
  timeout check 10s
  maxconn 20000
  
listen stats :9000
  mode http
  stats enable
  stats uri /
  
frontend front_route
  bind *:8443
  mode tcp
  
  tcp-request inspect-delay 5s
  tcp-request content accept if { req_ssl_hello_type 1 }
  
  # Here we use SNI to route the internal and external API requests
  acl subdomain_is_private req_ssl_sni -i <PRIVATE_MASTER_HOSTNAME>
  acl subdomain_is_public req_ssl_sni -i <PUBLIC_MASTER_HOSTNAME>
  
  # Route to specific backends for each type of traffic
  use_backend atomic-openshift-api-private if subdomain_is_private
  use_backend back_external if subdomain_is_public

# Loop back to atomic-openshift-api-public   
backend back_external
  mode tcp
  server public 127.0.0.1:9003

# Provides SSL termination using the master-server.pem found on the OSE master  
frontend atomic-openshift-api-public
  bind *:9003 ssl cert /etc/pki/tls/certs/master-server.pem ca-file /etc/pki/tls/certs/ca.pem
  # OPTIONAL: add client cert as a header in case it is needed.
  http-response add-header Client-Cert %{ssl_f_der,base64}
  default_backend atomic-openshift-api-public
  mode http
  log global
  option httplog
  
  # This is key to attribute API events to user sessions/tokens
  capture request header Authorization len 15
  
  capture cookie ssn len 25
  
backend atomic-openshift-api-public
  balance source
  mode http
  server master0 <MASTER1_IP>:8443 ssl check ca-file /etc/pki/tls/certs/ca.pem
  server master1 <MASTER2_IP>:8443 ssl check ca-file /etc/pki/tls/certs/ca.pem
  server master2 <MASTER3_IP>:8443 ssl check ca-file /etc/pki/tls/certs/ca.pem
  
backend atomic-openshift-api-private
  balance source
  mode tcp
  server master0 <MASTER1_IP>:8443 ssl check
  server master1 <MASTER2_IP>:8443 ssl check
  server master2 <MASTER3_IP>:8443 ssl check

----

In the file above, there are a few things to notice first. First of all, the main front-end block listening on port 8443. That doesn't change. What happens inside that block is a little more interesting. OpenShift provides a public URL for external API, and private URL for internal API interactions. These are defined in master-config.yaml. What we need to do here is SSL terminate the public API, while leaving the internal API as TLS pass-through. We can use SNI declarations in HAProxy to route this traffic based on the URL. Traffic tagged as external is routed to a special HAProxy front-end configured as a "feed-back loop" that listens on a special port 9003 to provide SSL terminiation and TLS pass-through respectively. SNI configuration is based on a procedure at http://serverfault.com/questions/662662/haproxy-with-sni-and-different-ssl-settings

If not already set, add the http_listen context to that port. 

----
# semanage port -a -t http_port -p tcp 9003
----

We need the SSL termination to be able to inspect the master traffic, so we copy the master.server.pem and master.server.key from /etc/origin/master/ on the OCP master nodes.
The backends have also been split to address the SSL termination. Notice the private and public backend, specifically that the public specifies CAs for each of the master servers. This CA should be configured with the same enterprise CA that you used when installing the masters.

Notice the HAProxy logging configuration under the global section.

----
log     127.0.0.1 local1
----

HAProxy uses rsyslog sockets to forward its logging facilities directly. This means all your haproxy events will appear on the local Rsyslog server under facility "local1". You can then configure rsyslog to output these logs to file if you wish in /etc/rsyslog.conf.

----
module(load="imudp")
input(type="imudp" port="514" address="127.0.0.1")
...
local1.*      -/var/log/haproxy.log
...
----

After making these changes to HAProxy, restart the service and you should start seeing your master API logging.
