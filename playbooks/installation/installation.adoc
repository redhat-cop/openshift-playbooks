---
---
= OpenShift Container Platform Smart Start Installation
Eric Sauer <esauer@redhat.com>
v2.0, 2016-08-18
:scripts_repo: https://github.com/rhtconsulting/rhc-ose
:toc: macro
:toc-title:

toc::[]

== Overview

The OpenShift Container Platform Smart Start is a reference architecture for a Highly Available installation of OpenShift. This document outlines the architecture of such an installation and walks through the installation process.

== Architecture

image::/images/ocp_smart_start_diagram.png[Smart Start Architecture]

The diagram above depicts the architecture of the Smart Start environment. It consists of 3 Masters and 5 Nodes. We further subdivide the nodes into two groups. Two of them get designated to run OpenShift's router and image registry. We refer to these as _Infrastructure Nodes_. The other three will run the actual application workloads. We call these _Application Nodes_ or _Compute Nodes_. In order to designate which nodes will do which job, we assign link:https://docs.openshift.com/container-platform/latest/architecture/core_concepts/pods_and_services.html#labels[labels] to them, which will be used by the OpenShift/Kubernetes link:https://docs.openshift.com/container-platform/latest/admin_guide/scheduler.html[scheduler] to assign certain workload types to each (i.e. "infra" workloads vs "primary" workloads). See the sample inventory file at the bottom of this doc to see how those labels are assigned.

NOTE: For more information on these concepts and components, see our link:/playbooks/fundamentals/building_blocks_openshift{outfilesuffix}[Building Blocks of OpenShift] doc.

== Before You Begin

Before jumping into an install of OpenShift, there are a few Architectural decisions and planning steps to go through. We attempt to outline them here.

=== Decide on Load Balancing Approach

In order to run a fully HA OpenShift cluster, load balancing will be required between both the 3 master hosts, and the 2 infrastructure node hosts. We recommend choosing one of the following options:

==== Option 1: Integrate with an External Loadbalancer (Recommended)

Even if you don't go this route initially, we highly recommend you plan to eventually bring an Enterprise-grade load balancer into your OpenShift environment. The primary reason we recommend this is for failover. Most Enterprise load balancers have built-in, proven capabilities to fail over a single VIP between two or more physical or virtual appliances. While this _can_ be done with software load balancers, like HAProxy, the resiliency and management simplicity just isn't quite the same.

To integrate with an external load balancer, at minimum, you'll need to create:

* A passthrough VIP and back-end pool for the Master hosts
* A passthrough VIP and back-end pool for the Infrastructure hosts

See our link:/playbooks/installation/load_balancing{outfilesuffix}[Integrating External Loadbalancers] guide for more details on this.

==== Option 2: Use the Integrated HAProxy Balancer

The OpenShift installer has the ability to configure a Linux host has a load balancer for your master servers. This has the disadvantage of being a single point of failure out of the box, and also doesn't meet the need for loadbalancing the infrastructure nodes. Additional, manual work will be needed post-install to rectify these shortcomings. Again, ultimately we recommend you go with Option 1, but this is a reasonable workaround so that you can continue with the install.

=== Provision Servers

The Smart Start architecture requires the following Servers(VMs) be provisioned:

* 3 _Masters_
  ** RHEL 7.2 minimal installation
  ** 24 GB Memory
  ** 8 Cores
  ** 60 GB for the partition or logical volume containing `/var`
  ** An additional 20 GB disk or logical volume mounted at `/var/log`
  ** An additional 10 GB disk or logical volume mounted at `/var/lib/etcd`
* 3 _Infrastructure Nodes_
  ** RHEL 7.2 minimal installation
  ** 24 GB Memory
  ** 6 Cores
  ** 40 GB for the partition or logical volume containing `/var`
  ** An additional 20 GB disk or logical volume mounted at `/var/log`
  ** An additional 20 GB disk or logical volume mounted at `/var/lib/origin`
  ** An additional 100 GB disk, unformatted and unmounted
* 3 _Application Nodes_
  ** 48 GB Memory
  ** 4 Cores
  ** 30 GB for the partition or logical volume containing `/var`
  ** An additional 10 GB logical volume mounted at `/var/log`
  ** An additional 20 GB logical volume mounted at `/var/lib/origin`
  ** An additional 100 GB disk, unformatted and unmounted
* (Optional) A Load Balancer host, if you plan to use Option 2 for Load Balancing, per the above section
  ** 2 cores
  ** 4 GB Memory
  ** 10 GB Root Drive

NOTE: We chose the resource sizing above based on some reasonable starting sizes for hosts based on our experiences in the field. Before building further environments, it is important to go through a real production sizing exercise. For more information on how we recommend going about this, see our link:/playbooks/operationalizing/environment_sizing{outfilesuffix}[Environment Sizing Guide].

NOTE: We recommend segregating local disks this way in order to provide more stability to the separate components that will write to these locations. For more information about why we configure local storage this way, read link:/playbooks/operationalizing/local_storage{outfilesuffix}[Considerations for Configuring Local Storage on OpenShift Hosts]

Additionally, the following is assumed about the environment OpenShift is being installed into:

* An external load balancer is available for use to load balance to the *Master* and *Infrastructure Node* servers
* An external DNS infrastructure is available for creating hostnames for the servers, as well as a wildcard entry for application traffic routing
* An external storage infrastructure capable of providing NFS volumes

NOTE: For more information, see the link:https://docs.openshift.com/container-platform/latest/install_config/install/prerequisites.html[Official Documentation regarding OpenShift Installation Prerequisites].

=== Configure DNS

OpenShift expects properly configured DNS in order to work properly. The assumption of this document is that you will be using some external DNS system (Corporate DNS) to do this. If you don't have an existing DNS system or don't plan to use it for the purpose of this setup, then you will have to create one. We recommend creating a bind server if this will be a permanent setup, or using DNSMasq as a temporary workaround if you plan to ultimately tie into a Corporate DNS in the future.

At a minimum the following needs to be true for all OpenShift hosts:

* Each server has a hostname, resolvable in DNS
* Each server's `hostname` command returns its fully qualified domain name (FQDN)
* Each server can ping all other servers via a hostname in DNS (no /etc/hosts entries)
* A wildcard DNS entry exists under a unique subdomain (i.e. `*.cloudapps.example.com`) that resolves to either the IP addresses (an A record) or the hostnames (a CNAME record) of the two _Infrastructure Nodes_

NOTE: For more information, see the link:https://docs.openshift.com/container-platform/latest/install_config/install/prerequisites.html#prereq-dns[Official Documentation Regarding DNS Requirements].


== Prepare Hosts for Install

Overall requirements for Installing OpenShift are very simple:

* Install Red Hat provided Red Hat Enterprise Linux 7.x image
** If the environment uses a custom image, we recommend testing the OSE install with the Red Hat provided image first, then incrementally adding environment-specific image customizations with an eye toward the following:
*** Assess whether the image customization is still appropriate and necessary (to avoid changes inherited from earlier circumstances that are no longer relevant)
*** Test each customization to make sure it allows normal OSE operation and mitigate if necessary
* Subscribe to the following channels on the Red Hat Customer Portal (if environment uses Satellite, custom yum repos, or other mechanisms, these channels may have different organization and/or labels)
** rhel-7-server-rpms
** rhel-7-server-extras-rpms
** rhel-7-server-optional-rpms
** rhel-7-server-ose-3.x-rpms
* If you plan to Install HA Masters, an additional channel is needed on Master Nodes
** rhel-7-for-ha-server-rpms
* Install the following extra packages: `yum install NetworkManager wget git net-tools bind-utils iptables-services bridge-utils`
* Optional: install the following diagnostic tools: `yum install lsof strace nc telnet mlocate`
* Fully update all packages: `yum -y update`
* Ensure NetworkManager is running and enabled
** `systemctl status NetworkManager`
** `systemctl enable NetworkManager`
** `systemctl start NetworkManager`
* Ensure time is in sync on all nodes
** chrony: is the default package in RHEL 7.
----
    yum install -y chrony      # to install
    systemctl enable chronyd   # to enable
    systemctl start chronyd    # to start
    chronyc tracking           # To get information about the main time reference
    chronyc sources -v         # equivalent information to the ntpq
    ntpdate pool.ntp.org       # To quickly synchronize a server`
----
* Sync SSH keys from masters to all nodes (Here's how: https://docs.openshift.com/container-platform/latest/install_config/install/prerequisites.html#ensuring-host-access)
* Ensure company CA's are installed on all the nodes in the OCP Cluster
** Copy company CA's here `/etc/pki/ca-trust/source/anchors`
** Install company CA's `update-ca-trust`

NOTE: For more information, see the link:https://docs.openshift.com/container-platform/latest/install_config/install/prerequisites.html#host-preparation[Official Documenation for Host Preparation].

== Configure and Run the Ansible Installer

We highly recommend using the link:https://docs.openshift.com/container-platform/latest/install_config/install/advanced_install.html#installing-ansible[Advanced Installation method using Ansible] for basically any multi-node installation. The OpenShift Quick Installer is available and useful for quick demos and short-lived installs, but does not support the customization needed to install in many real environments.

The instructions in the Installer Guide will get you through most basic installs, but there are few additional things to know and be aware of to really understand the installer.

=== The Ansible Inventory File

While the Install Guide shows some basic examples for link:https://docs.openshift.com/container-platform/latest/install_config/install/advanced_install.html#configuring-ansible[Configuring Ansible Hosts], there are many more options and variables that can be used to further customize your install. We attempt to break down a few of the common ones here.

==== Explicitly Set Hostnames and IPs

In many cloud environments, it's common for you to assign hostnames to your hosts that differ from those that hosts were provisioned with. Ansible attempts to auto guess hostnames for your master and node hosts, but often in cloud environments these hostnames are set in multiple locations. Furthermore, it's not always obvious in cloud-enabled hosts what IPs OpenShift should be using. For this reason, the Installer allows you to explicitly set these using variables in your hosts file.

----
[masters]
master.ose.example.com openshift_hostname=master.ose.example.com openshift_public_ip=10.3.4.5
----

==== Configure Corporate Proxy Settings

If your OpenShift environment will need to go through a Web Proxy, you'll want to configure that proxy information in your inventory file.

----
openshift_http_proxy=http://proxy.example.com:1234
openshift_https_proxy=http://proxy.example.com:1235
openshift_no_proxy=localhost,127.0.0.1 # The Installer will automatically append hosts and ips of the masters and nodes to this list
----

==== Example Inventory File

Ultimately, your inventory file for the OpenShift installer should look something like this:

----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
# Uncomment the following for Load Balancing Option 2
#lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
deployment_type=openshift-enterprise

# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Native high availbility cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=master-external-lb.example.com
openshift_master_cluster_public_hostname=master-internal-lb.example.com

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
master1.example.com
master2.example.com
master3.example.com

# Specify load balancer host
# Uncomment these two lines if you didn't integrate with an external LB
#[lb]
#lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com openshift_node_labels="{'region': 'masters', 'zone': 'default'}"
infranode[1:3].example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----
