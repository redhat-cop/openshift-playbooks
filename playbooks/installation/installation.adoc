---
---
= OpenShift Enterprise 3.2 Installation
Eric Sauer <esauer@redhat.com>
Brent Roskos <broskos@redhat.com>
Nick Poyant <npoyant@redhat.com
v2.0, 2016-06-13
:scripts_repo: https://github.com/rhtconsulting/rhc-ose
:toc: macro
:toc-title:

toc::[]

== Prereqs

Official Documentation: https://docs.openshift.com/enterprise/3.2/install_config/install/prerequisites.html

=== DNS

If the organization has the ability to add an A record to the corporate DNS, supporting wildcard name resolution to the OpenShift environment, that DNS server should be utilized.

If adding those entries is problematic, a quick DNS solution for OpenShift is to use dnsmasq.

=== Node Sizing

|===============
| Masters |
Physical or virtual system, or an instance running on a public or private IaaS.

Base OS: RHEL 7.1 or later with "Minimal" installation option, or RHEL Atomic Host 7.2.4 or later.

2 vCPU.

Minimum 8 GB RAM.

Minimum 30 GB hard disk space for the file system containing /var/.

| Nodes |
Physical or virtual system, or an instance running on a public or private IaaS.

Base OS: RHEL 7.1 or later with "Minimal" installation option, or RHEL Atomic Host 7.2.4 or later.

NetworkManager 1.0 or later

1 vCPU.

Minimum 8 GB RAM.

Minimum 15 GB hard disk space for the file system containing /var/.

An additional minimum 15 GB unallocated space to be used for Dockerâ€™s storage back end; see Configuring Docker Storage below.
|===============

==== Number of Nodes & Overall Environment Size

The Smart Start Node configuration consists of 3 Masters, 2 Infrastructure Nodes, and 3 Application Nodes.

When deciding how to size nodes, it's helpful to have an idea of the final size of your environment. This is important because knowing the total amount of resources your workloads will consume will help to dictate the amount of capacity loss your environment can handle. It's a good rule of thumb for environment sizing is to take the max expected total workload size and add 30% to that. This leaves about 10% for overhead on your nodes, plus 20% extra to account for node failure.

NOTE: This applies to _real_ environments (i.e. _Dev_, _QA_, _Prod_) where we care about handling failure. Lab or sandbox environments will work just fine using the minimal recommendations.

==== Type of Workload 
// This section needs to be updated to reflect 3.2 information.
Depending on the types of Workloads you expect to run in OpenShift, you may want to cater certain nodes to certain workload types. For example, computational workloads may want to run on nodes that have a higher CPU-to-Memory ratio (say 1 Core to 2 GB), while legacy Java applications that have a large starting memory footprint but low transactions may be more efficiently run with higher memory and low CPU counts. Nodes in OpenShift do not all need to be the same size, and using Topology Rules within the Scheduler, you can create separate groups of Nodes catered to different workload profiles.

If you expect to support a healthy mix of different workload types and sizes, then its most likely going to be best to pick a balanced middle ground and allow the Scheduler to fit workloads where they will be most appropriate.

==== Node Limit
// This section needs to be updated to reflect 3.2 information.
.Update Needed.
****
This section needs to be updated to reflect 3.2 information.
****

Kubernetes, and therefore OpenShift has a logical limit to the number of nodes that can be provisioned in a single cluster (single OpenShift installation). We have seen test configurations of roughly 250 nodes. This is expected to increase with future releases (3.1, etc.).

The logical limit for Nodes in a cluster might sway administrators of large clusters to increase node size in order to increase the total capacity a Cluster can support

=== Firewall Considerations
Ansible playbook will manage the host-based firewall configuration.  If you have a segregated network topology (or if your environment has host-to-host external firewall rules), you should consider the following: 

[width="40%",frame="topbot",options="header"]
|========================================================
| Port | Protocol | Source-to-Dest | Description 
| 4789 | UDP      | Node-to-Node          | required between nodes for SDN communication between pods on separate hosts  
| 53   | TCP/UDP  | Nodes-to-Master       | On openshift master server. Which provides DNS services within the environment  
| 4789 | UDP      | Nodes-to-Master       | required between nodes for SDN communication between pods on separate hosts  
| 53   | TCP/UDP  | Master-to-Master      | On openshift master server. Which provides DNS services within the environment  
| 4789 | UDP      | Master-to-Master      | required between nodes for SDN communication between pods on separate hosts  
| 4001 | TCP/UDP  | Master-to-Master      | Master: ETCD. accepts changes in state  
| 7001 | TCP      | Master-to-Master      | Etcd requires 7001 between masters as well, which is used for leader election and peering connections.  
| 10250 | TCP      | Master-to-Nodes      | on the minons(Kublet), master proxies to nodes via kubelet for things like `oc logs` or `oc exec`
| 8443 | TCP      | External-to-Master(s) | To allow API updates via cli, GUI or REST  
|========================================================

You should also review the following KB https://access.redhat.com/solutions/1520653

==== Questions to Ask

. What is the total workload landscape?
. What is the CPU and memory footprint of to-be-migrated workloads?
. What is the platform type (bare metal, virtualized, IaaS)? (Failure is more difficult to handle in bare metal circumstances and may require more nodes)
. Can all workloads run on the same nodes or do you require segregation?

=== Storage Considerations

Without the proper protections in place (through either Disk partitioning or volume groups), we leave ourselves at risk that a component or service running on the host OS could potentially fill the disk, preventing either of these from writing to disk and corrupting the OSE install.

==== OpenShift Storage Requirements

There are two main components of OpenShift that require specialized storage considerations to ensure environment stability. These components are the OpenShift DataStore (etcd) and the Container Platform (Docker).

===== DataStore (etcd)

OpenShift uses a key-value pair datastore called etcd to manage environment state. Etcd writes this data to `/var/lib/openshift`. While the data needs of the datastore are by no means large, it is important that it be given dedicated storage in order to protect it from other items (especially logs) filling up the disk, which could cause corruption of the etcd data.

IMPORTANT: When running multiple instances of etcd in a cluster configuration, etcd runs as a standalone process, rather than being embedded in the `openshift-master` service. When this is the case, etcd writes to `/var/lib/etcd` rather than `/var/lib/openshift`, and therefore the configurations should be adjusted accordingly.

The size of the partition can be relatively small. For a lab or sandbox environment, 2GB should be plenty of disk. For larger, more permanent environments which will support multiple teams/users, 5 - 10GB may be needed.

====== Backups, Shared Storage & Replication

Etcd datastores can be clustered in order to prevent loss of data in the event of a failure. In this case etcd handles its own data replication, so there is no need for shared storage or external data replication. Standard backup practices can and should be observed here, but nothing more is needed.

===== Container Platform (Docker)

Docker is the current container platform used by OpenShift. Docker uses local storage on each node to store images and active containers. The amount of storage needed depends on the size of the node and the number and size of the containers expected to be supported by each node. It would not be uncommon for Nodes to support anywhere from 20 - 100 containers depending on the environment, and container images can be sizeable. We recommend planning for a reasonably large image size of 500mb. That means a partition or volume size of 20 - 50GB is required per node.

Docker's data directories live in `/var/lib/docker`, so the disk space should be allocated to that mount point. As with the OpenShift DataStore, it is highly recommended to give Docker a dedicated partition or volume so as to protect it from over-crowding by logging or other administrative services.

NOTE: This section is referring to local storage used by Docker to run containers on a host. This *DOES NOT* refer to running a Docker registry.

====== Backups, Shared Storage & Replication

Nodes are considered stateless in a Kubernetes architecture, meaning that in the event of a loss of a Node, the Scheduler will immediately replace lost containers elsewhere in the environment. Therefore, no specialized sharing or replication is required at the node level to account for loss.

====== Configuration Considerations

The `docker` package ships with a command for setting up docker storage, `docker-storage-setup`. This script expects us to create a volume group for it in order to point it to the storage we've allocated.

==== Storage Configuration Implementation

===== Local Disk Config

Create the following partitions (disk partitions or LVMs) on the system disk image:

* Boot partition (`/boot`)
* OS Root (`/`)
* etcd storage (`/var/lib/openshift` or `/var/lib/etcd`)
* Log storage (`/var/log`) - Optional, but recommended

===== Docker Storage Config

The OpenShift docs discuss link:https://docs.openshift.com/enterprise/3.2/install_config/install/prerequisites.html#configuring-docker-storage[three different options for configuring docker storage]. We consider options A and B to be "production ready".

For option A: Attach an additional volume or block device to each node for Docker storage (20-50 GB)

For option B: Leave unallocated space (20 - 50 GB) on your local disk. From the unallocated space, create the following volume group:

* `docker-vg`

TIP: For more information and examples about docker-storage-setup options, see link:https://access.redhat.com/articles/1492923[Managing Storage with Docker Formatted Containers on Red Hat Enterprise Linux and Red Hat Enterprise Linux Atomic Host] in the Red Hat Knowledge Base

===== Example: Configuring Host Storage with Docker Storage Option A

To account for the above DataStore & Container storage needs, we are going to attach a single 25 GB volume and partition it to accommodate our needs. Here is a sample vdb layout for the disk:

NOTE: The following must be done on ALL NODES, including the Master. However, if you are not going to be using the master as a node, then a minimal volume size can be used, as no containers will be hosted. Also, for NODES that are NOT MASTERS, the second DataStore partition is not needed.

[source,bash]
----
# NOTE: actual numeric values will be different for each running system, but the steps outlined should be the same for all

>> fdisk /dev/vdb

Command (m for help): n <enter>
Partition type:
   p   primary (0 primary, 0 extended, 4 free)
   e   extended
Select (default p): p <enter>
Partition number (1-4, default 1):  <enter>
First sector (2048-16777215, default 2048): <enter>
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-16777215, default 16777215): +20G <enter>
Partition 1 of type Linux and of size 20 GiB is set

Command (m for help): n <enter>
Partition type:
   p   primary (1 primary, 0 extended, 3 free)
   e   extended
Select (default p): p <enter>
Partition number (2-4, default 2): <enter>
First sector (12584960-16777215, default 12584960): <enter>
Using default value 12584960
Last sector, +sectors or +size{K,M,G} (12584960-16777215, default 16777215): +2G <enter>
Partition 2 of type Linux and of size 2 GiB is set

Command (m for help): t <enter>
Partition number (1,2, default 2): 1 <enter>
Hex code (type L to list all codes): 8e <enter>
Changed type of partition 'Linux' to 'Linux LVM'

Command (m for help): t <enter>
Partition number (1,2, default 2): 2 <enter>
Hex code (type L to list all codes): 8e <enter>
Changed type of partition 'Linux' to 'Linux LVM'

Command (m for help): w <enter>
The partition table has been altered!

Calling ioctl() to re-read partition table.
Syncing disks.
----

We then create the volume group, install docker, and run `docker-storage-setup`.

[source,bash]
----
pvcreate /dev/vdb1
vgcreate vg-docker /dev/vdb1

cat << EOF > /etc/sysconfig/docker-storage-setup
VG=vg-docker
SETUP_LVM_THIN_POOL=yes
EOF

# Let docker setup the storage based on the above config file
docker-storage-setup
----

Finally, the DataStore, we setup the volume group, create data directory, and mount it.
[source,bash]
----
pvcreate /dev/vdb2
vgcreate vg-openshift /dev/vdb2
lvcreate -l 100%FREE -n lv-ose vg-openshift

mkfs.xfs -q -f /dev/vg-openshift/lv-ose

mkdir -p /var/lib/openshift
echo "/dev/vg-openshift/lv-ose        /var/lib/openshift              xfs defaults 0 0" >> /etc/fstab

mount -a
----

===== Example: Configuring Host Storage with Docker Storage Option B

In this example, we have a device (vdc) with available space that we will dedicate to vg-docker which will be used by docker-storage-setup (identified in /etc/
sysconfig/docker-storage-setup)

First, review the current configuration of the device
[source,bash]
----
parted /dev/vdc print
Model: Virtio Block Device (virtblk)
Disk /dev/vdc: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start   End     Size    File system  Name     Flags
 1      1049kB  10.0GB  9999MB               primary  lvm
----

Add a partition, using the remainder of the device.
[source,bash]
----

parted /dev/vdc mkpart primary ext3 10g 100% set 2 lvm on
Information: You may need to update /etc/fstab.

parted /dev/vdc print
Model: Virtio Block Device (virtblk)
Disk /dev/vdc: 21.5GB
Sector size (logical/physical): 512B/512B
Partition Table: gpt
Disk Flags:

Number  Start   End     Size    File system  Name     Flags
 1      1049kB  10.0GB  9999MB               primary  lvm
 2      10.0GB  21.5GB  11.5GB               primary  lvm
----

Create the PV and a VG using that PV.
[source,bash]
----
pvcreate /dev/vdc2
  Physical volume "/dev/vdc2" successfully created
vgcreate vg-docker /dev/vdc2
  Volume group "vg-docker" successfully created
----

Update the docker-storage-setup file that is used as a guideline and run the docker-storage-setup command.
[source,bash]
----
cat << EOF > /etc/sysconfig/docker-storage-setup
VG=vg-docker
SETUP_LVM_THIN_POOL=yes
EOF

# Let docker setup the storage based on the above config file
docker-storage-setup
----

== Host Preparation

Official Documenation for link:https://docs.openshift.com/enterprise/3.2/install_config/install/prerequisites.html#host-preparation[Host Preparation].

Overall requirements for Installing OpenShift are very simple:

* Install Red Hat provided Red Hat Enterprise Linux 7.1 image
** If the environment uses a custom image, we recommend testing the OSE install with the Red Hat provided image first, then incrementally adding environment-specific image customizations with an eye toward the following:
*** Assess whether the image customization is still appropriate and necessary (to avoid changes inherited from earlier circumstances that are no longer relevant)
*** Test each customization to make sure it allows normal OSE operation and mitigate if necessary
* Subscribe to the following channels on the Red Hat Customer Portal (if environment uses Satellite, custom yum repos, or other mechanisms, these channels may have different organization and/or labels)
** rhel-7-server-rpms
** rhel-7-server-extras-rpms
** rhel-7-server-optional-rpms
** rhel-7-server-ose-3.2-rpms

* Install the following extra packages: `yum install wget git net-tools bind-utils iptables-services bridge-utils`
* Optional: install the following diagnostic tools: `yum install lsof strace nc telnet`
* Fully update all packages: `yum -y update`
* Configure Storage according to <<storage-considerations>>
* Sync SSH keys from masters to all nodes. Here's a script to do this:

[source, bash]
----
# nodes="ose-installer master00 master01 master02 infranode00 infranode01 nfs00 app-node00 app-node01 app-node02"
# for node in $nodes; do ssh-copy-id $node; done
---- 

== Ansible Installer

We highly recommend using the link:https://docs.openshift.com/enterprise/3.2/install_config/install/advanced_install.html[Advanced Installation method using Ansible] for basically any multi-node installation. The OpenShift Quick Installer is available and useful for quick demos and short-lived installs, but does not support the customization needed to install in many real environments.

The instructions in the Installer Guide will get you through most basic installs, but there are few additional things to know and be aware of to really understand the installer.

=== Installer Source Code

The Ansible Installer source code is downloaded from link:https://github.com/openshift/openshift-ansible[GitHub]. At the time of writing this document, the docs instruct you to pull down the Master (main) branch of the source code. Changes and bug fixes are merged into this repository on a daily basis, so the installer does not follow the same release cycle that the OpenShift bits do. This means that there are chances that new bug fixes that are merged in could potentially break the installer which had just been working for you the day before. It is important to remember this when you are planning a large environment deployment.

IMPORTANT: Our recommendation is to keep a copy of the installer you use to install your environment for re-use when adding nodes or trying to replicate your environment build. When it's time to upgrade to do a new release of the env, you should then pull down the latest installer as a starting point.

=== The Ansible Hosts File

While the Install Guide shows some basic examples for link:https://docs.openshift.com/enterprise/3.2/install_config/install/advanced_install.html#configuring-ansible[Configuring Ansible Hosts], there are many more options and variables that can be used to further customize your install. We attempt to break down a few of the common ones here.

==== Explicitly Set Hostnames and IPs

In many cloud environments, it's common for you to assign hostnames to your hosts that differ from those that hosts were provisioned with. Ansible attempts to auto guess hostnames for your master and node hosts, but often in cloud environments these hostnames are set in multiple locations. Furthermore, it's not always obvious in cloud-enabled hosts what IPs OpenShift should be using. For this reason, the Installer allows you to explicitly set these using variables in your hosts file.

----
[masters]
master.ose.example.com openshift_hostname=master.ose.example.com openshift_public_ip=10.3.4.5
----

==== Smart Start/HA Example Host File
This host file contains a core set of options to get your HA build up and running with a local HA Proxy to mimic a load balancer. 
----
# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
deployment_type=openshift-enterprise

# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

# Native high availbility cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.
openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-cluster.example.com
openshift_master_cluster_public_hostname=openshift-cluster.example.com

# override the default controller lease ttl
#osm_controller_lease_ttl=30

# host group for masters
[masters]
master1.example.com
master2.example.com
master3.example.com

# host group for etcd
[etcd]
etcd1.example.com
etcd2.example.com
etcd3.example.com

# Specify load balancer host
[lb]
lb.example.com

# host group for nodes, includes region info
[nodes]
master[1:3].example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
----

In an effort to streamline the file or if you have a large number of nodes, another method of listing multiple nodes in a host file:

----
master1:3.example.com
infranode1:2.example.com
node1:3.example.com
----

===== DNS Subdomain
----
osm_default_subdomain=cloudapps.example.com
----

===== Custom Network Ranges
----
openshift_master_portal_net=172.X.X.0/20
osm_cluster_network_cidr=172.X.Y.0/18
----

===== Node Labeling
----
infranode1:2.example.com openshift_node_labels="{'region':'infra','zone':'default'}"
app-node1:3.example.com openshift_node_labels="{'region':'primary','zone':'default'}"
----

===== Master Scheduable Flag
----
master1:3.example.com openshift_node_labels="{'zone':'default'}" openshift_schedulable=false
----

===== Default Region
----
osm_default_node_selector='region=primary'
----

===== Port Designation
----
openshift_master_console_port=443
openshift_master_api_port=443
----

===== Deploying Custom Certificates
----
openshift_hosted_router_certificates=[{"certfile": "/path/to/certs/splat.example.com.crt", "keyfile": "/path/to/certs/splat.example.com.key"}]
----

===== LDAP Identity Provider
-----
openshift_master_identity_providers=[{'name': 'OrgName_ldap', 'challenge': 'true', 'login': 'true', 'kind': 'LDAPPasswordIdentityProvider', 'attributes': {'id': ['sAMAccountName'], 'email': ['mail'], 'name': ['cn'], 'preferredUsername': ['sAMAccountName']}, 'bindDN': 'cn=LDAP_Account,ou=Resource,dc=example,dc=local', 'bindPassword': 'Ld@pP@$$w0rd', 'ca': 'org-ca.crt', 'insecure': 'false', 'url': 'ldaps://server_name.org_name.local:636/DC=org,DC=local?sAMAccountName?sub?(&(objectClass=user)(memberOf=CN=group_name,OU=Resource,DC=org_name,DC=local))'}]
-----

==== Registry hosted on Netapp Provided NFS
----
openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_volume_name=VolumeName
openshift_hosted_registry_storage_host=FileServer.example.com
openshift_hosted_registry_storage_nfs_options='*(rw,all_squash)'
openshift_hosted_registry_storage_nfs_directory=/
openshift_hosted_registry_storage_volume_size=95G
----

.Feedback or Contribution Needed
****
Need to add DNS subdomain, node labeling, master schedulable flag
****

== Automating the Install

To launch the HA installer you will use an ansible playbook that will point to your custom hosts file that is tailored to your environment.

----
# ansible -i /path/to/hosts/file/HostFile --become /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
----

Here are some adhoc ansible commands that you can use to make configuring your environment easier:

----
# ansible -i /path/to/hosts/file/HostFile nodes --become -m copy -a "src=/path/to/file/to/copy/file.txt dest=/path/to/copy/location/"
----

----
# ansible -i /path/to/hosts/file/HostFile nodes --become -m shell -a "subscription-manager status"
----

----
# ansible -i /path/to/hosts/file/HostFile nodes --become -m shell -a "yum install docker"
----

.Feedback or Contribution Needed
****
Things to talk about

* Scripting options
** Ansible
** Bash (show osc-install script)
****

== Troubleshooting

.Feedback or Contribution Needed
****
Things to talk about

* Hostname issue
****

== Parking Lot Items

.Feedback or Contribution Needed
****
This is a list of items that we would like contributions on:

* HA Masters need High Availability Add-On
* Adding Nodes
****
